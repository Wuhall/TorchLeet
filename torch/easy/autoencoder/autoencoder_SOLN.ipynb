{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Train an Autoencoder for Anomaly Detection\n",
    "\n",
    "### Problem Statement\n",
    "You are tasked with implementing an **autoencoder** model for anomaly detection. The model will be trained on the **MNIST dataset**, and anomalies will be detected based on the reconstruction error. The autoencoder consists of an encoder to compress the input and a decoder to reconstruct the image. The difference between the original image and the reconstructed image will be used to detect anomalies.\n",
    "\n",
    "你的任务是实现一个用于异常检测的**自动编码器**模型。该模型将在**MNIST**数据集上进行训练，并根据重建误差检测异常。该自动编码器由一个用于压缩输入的编码器和一个用于重建图像的解码器组成。原始图像和重建图像之间的差异将用于检测异常。\n",
    "\n",
    "### Requirements\n",
    "1. **Define the Autoencoder Architecture**:\n",
    "   - **Encoder**:\n",
    "     - Implement a series of convolutional layers followed by max-pooling layers.\n",
    "     - The encoder should progressively reduce the spatial dimensions of the input image, capturing the most important features.\n",
    "   - **Decoder**:\n",
    "     - Implement a series of transposed convolutional layers (also known as deconvolutional layers) to upsample the compressed representation back to the original image size.\n",
    "     - Use a **Sigmoid activation** function in the final layer to ensure that the output pixel values are between 0 and 1.\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   - Implement the forward method where the input image is passed through the encoder to obtain a compressed representation, followed by passing it through the decoder to reconstruct the image.\n",
    "\n",
    "3. **定义自动编码器架构**：\n",
    "- **编码器**：\n",
    "- 实现一系列卷积层，然后是最大池化层。\n",
    "- 编码器应逐步降低输入图像的空间维度，以捕捉最重要的特征。\n",
    "- **解码器**：\n",
    "- 实现一系列转置卷积层（也称为反卷积层），将压缩表示上采样回原始图像大小。\n",
    "- 在最后一层使用**Sigmoid 激活函数**，以确保输出像素值介于 0 和 1 之间。\n",
    "\n",
    "4. **前向传播**：\n",
    "- 实现前向传播方法，将输入图像通过编码器获得压缩表示，然后通过解码器重建图像。\n",
    "\n",
    "\n",
    "\n",
    "### Constraints\n",
    "- The autoencoder should work on the MNIST dataset, which consists of 28x28 grayscale images.\n",
    "- Ensure that the output of the decoder matches the original image size.\n",
    "- Use **Sigmoid activation** in the final layer to constrain the output pixel values between 0 and 1.\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>💡 Hint</summary>\n",
    "  Focus on the encoder to downsample the input and the decoder to upsample and reconstruct the image.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Downsample to 14x14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Downsample to 7x7\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 转置卷积实现上采样\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),  # To keep pixel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.9224\n",
      "Epoch [2/10], Loss: 0.9234\n",
      "Epoch [3/10], Loss: 0.9281\n",
      "Epoch [4/10], Loss: 0.9225\n",
      "Epoch [5/10], Loss: 0.9266\n",
      "Epoch [6/10], Loss: 0.9240\n",
      "Epoch [7/10], Loss: 0.9225\n",
      "Epoch [8/10], Loss: 0.9276\n",
      "Epoch [9/10], Loss: 0.9221\n",
      "Epoch [10/10], Loss: 0.9246\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for images, _ in train_loader:\n",
    "        # Forward pass\n",
    "        reconstructed = model(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies using reconstruction error\n",
    "threshold = 0.1  # Define a threshold for anomaly detection\n",
    "model.eval()\n",
    "anomalies = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        reconstructed = model(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "        \n",
    "        # If reconstruction error exceeds the threshold, mark it as an anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly image shape: torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize anomalies\n",
    "if anomalies:\n",
    "    # Select the first anomaly and remove the channel dimension for visualization\n",
    "    anomaly_image = anomalies[0][0].squeeze()  # Remove the channel dimension (1)\n",
    "    print(f\"Anomaly image shape: {anomaly_image.shape}\")  # Optional: Check the shape of the image\n",
    "    plt.imshow(anomaly_image.cpu().numpy(), cmap='gray')  # Convert tensor to NumPy array for visualization\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No anomalies detected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
