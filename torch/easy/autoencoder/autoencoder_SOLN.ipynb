{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Train an Autoencoder for Anomaly Detection\n",
    "\n",
    "### Problem Statement\n",
    "You are tasked with implementing an **autoencoder** model for anomaly detection. The model will be trained on the **MNIST dataset**, and anomalies will be detected based on the reconstruction error. The autoencoder consists of an encoder to compress the input and a decoder to reconstruct the image. The difference between the original image and the reconstructed image will be used to detect anomalies.\n",
    "\n",
    "‰Ω†ÁöÑ‰ªªÂä°ÊòØÂÆûÁé∞‰∏Ä‰∏™Áî®‰∫éÂºÇÂ∏∏Ê£ÄÊµãÁöÑ**Ëá™Âä®ÁºñÁ†ÅÂô®**Ê®°Âûã„ÄÇËØ•Ê®°ÂûãÂ∞ÜÂú®**MNIST**Êï∞ÊçÆÈõÜ‰∏äËøõË°åËÆ≠ÁªÉÔºåÂπ∂Ê†πÊçÆÈáçÂª∫ËØØÂ∑ÆÊ£ÄÊµãÂºÇÂ∏∏„ÄÇËØ•Ëá™Âä®ÁºñÁ†ÅÂô®Áî±‰∏Ä‰∏™Áî®‰∫éÂéãÁº©ËæìÂÖ•ÁöÑÁºñÁ†ÅÂô®Âíå‰∏Ä‰∏™Áî®‰∫éÈáçÂª∫ÂõæÂÉèÁöÑËß£Á†ÅÂô®ÁªÑÊàê„ÄÇÂéüÂßãÂõæÂÉèÂíåÈáçÂª∫ÂõæÂÉè‰πãÈó¥ÁöÑÂ∑ÆÂºÇÂ∞ÜÁî®‰∫éÊ£ÄÊµãÂºÇÂ∏∏„ÄÇ\n",
    "\n",
    "### Requirements\n",
    "1. **Define the Autoencoder Architecture**:\n",
    "   - **Encoder**:\n",
    "     - Implement a series of convolutional layers followed by max-pooling layers.\n",
    "     - The encoder should progressively reduce the spatial dimensions of the input image, capturing the most important features.\n",
    "   - **Decoder**:\n",
    "     - Implement a series of transposed convolutional layers (also known as deconvolutional layers) to upsample the compressed representation back to the original image size.\n",
    "     - Use a **Sigmoid activation** function in the final layer to ensure that the output pixel values are between 0 and 1.\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   - Implement the forward method where the input image is passed through the encoder to obtain a compressed representation, followed by passing it through the decoder to reconstruct the image.\n",
    "\n",
    "3. **ÂÆö‰πâËá™Âä®ÁºñÁ†ÅÂô®Êû∂ÊûÑ**Ôºö\n",
    "- **ÁºñÁ†ÅÂô®**Ôºö\n",
    "- ÂÆûÁé∞‰∏ÄÁ≥ªÂàóÂç∑ÁßØÂ±ÇÔºåÁÑ∂ÂêéÊòØÊúÄÂ§ßÊ±†ÂåñÂ±Ç„ÄÇ\n",
    "- ÁºñÁ†ÅÂô®Â∫îÈÄêÊ≠•Èôç‰ΩéËæìÂÖ•ÂõæÂÉèÁöÑÁ©∫Èó¥Áª¥Â∫¶Ôºå‰ª•ÊçïÊçâÊúÄÈáçË¶ÅÁöÑÁâπÂæÅ„ÄÇ\n",
    "- **Ëß£Á†ÅÂô®**Ôºö\n",
    "- ÂÆûÁé∞‰∏ÄÁ≥ªÂàóËΩ¨ÁΩÆÂç∑ÁßØÂ±ÇÔºà‰πüÁß∞‰∏∫ÂèçÂç∑ÁßØÂ±ÇÔºâÔºåÂ∞ÜÂéãÁº©Ë°®Á§∫‰∏äÈááÊ†∑ÂõûÂéüÂßãÂõæÂÉèÂ§ßÂ∞è„ÄÇ\n",
    "- Âú®ÊúÄÂêé‰∏ÄÂ±Ç‰ΩøÁî®**Sigmoid ÊøÄÊ¥ªÂáΩÊï∞**Ôºå‰ª•Á°Æ‰øùËæìÂá∫ÂÉèÁ¥†ÂÄº‰ªã‰∫é 0 Âíå 1 ‰πãÈó¥„ÄÇ\n",
    "\n",
    "4. **ÂâçÂêë‰º†Êí≠**Ôºö\n",
    "- ÂÆûÁé∞ÂâçÂêë‰º†Êí≠ÊñπÊ≥ïÔºåÂ∞ÜËæìÂÖ•ÂõæÂÉèÈÄöËøáÁºñÁ†ÅÂô®Ëé∑ÂæóÂéãÁº©Ë°®Á§∫ÔºåÁÑ∂ÂêéÈÄöËøáËß£Á†ÅÂô®ÈáçÂª∫ÂõæÂÉè„ÄÇ\n",
    "\n",
    "\n",
    "\n",
    "### Constraints\n",
    "- The autoencoder should work on the MNIST dataset, which consists of 28x28 grayscale images.\n",
    "- Ensure that the output of the decoder matches the original image size.\n",
    "- Use **Sigmoid activation** in the final layer to constrain the output pixel values between 0 and 1.\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Hint</summary>\n",
    "  Focus on the encoder to downsample the input and the decoder to upsample and reconstruct the image.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Downsample to 14x14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Downsample to 7x7\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            # ËΩ¨ÁΩÆÂç∑ÁßØÂÆûÁé∞‰∏äÈááÊ†∑\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),  # To keep pixel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.9224\n",
      "Epoch [2/10], Loss: 0.9234\n",
      "Epoch [3/10], Loss: 0.9281\n",
      "Epoch [4/10], Loss: 0.9225\n",
      "Epoch [5/10], Loss: 0.9266\n",
      "Epoch [6/10], Loss: 0.9240\n",
      "Epoch [7/10], Loss: 0.9225\n",
      "Epoch [8/10], Loss: 0.9276\n",
      "Epoch [9/10], Loss: 0.9221\n",
      "Epoch [10/10], Loss: 0.9246\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for images, _ in train_loader:\n",
    "        # Forward pass\n",
    "        reconstructed = model(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies using reconstruction error\n",
    "threshold = 0.1  # Define a threshold for anomaly detection\n",
    "model.eval()\n",
    "anomalies = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        reconstructed = model(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "        \n",
    "        # If reconstruction error exceeds the threshold, mark it as an anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly image shape: torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize anomalies\n",
    "if anomalies:\n",
    "    # Select the first anomaly and remove the channel dimension for visualization\n",
    "    anomaly_image = anomalies[0][0].squeeze()  # Remove the channel dimension (1)\n",
    "    print(f\"Anomaly image shape: {anomaly_image.shape}\")  # Optional: Check the shape of the image\n",
    "    plt.imshow(anomaly_image.cpu().numpy(), cmap='gray')  # Convert tensor to NumPy array for visualization\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No anomalies detected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
